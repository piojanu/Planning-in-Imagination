\section{Introduction}

\editnote{* What is RL and model-free RL...}
Reinforcement learning, a subfield of artificial intelligence (AI), formalise rather the most obvious and common among animals learning strategy. It is learning how to achieve predefined goals through interaction with an environment \cite{Book.RLAI}. Progress has been made in developing capable agents for numerous domains using deep neural networks in conjunction with model-free reinforcement learning \cite{Algo.Rainbow}\cite{Algo.A3C}\cite{Algo.PPO}, where raw observations directly map to agent's actions. However, current state-of-the-art approaches are very sample inefficient, they sometimes require tens or even hundreds of millions of interactions with the environment \cite{Benchmark.RevisitingALE}, and lack the behavioural flexibility of human intelligence, hence the resulting policies poorly generalize to novel tasks in the same environment.

\editnote{* Model-based RL with its benefits...}
The other branch of reinforcement learning algorithms, called model-based reinforcement learning, aims to address these shortcomings by endowing agents with a model of the world. There are many ways of using the model: one can use the model for data augmentation for model-free methods \cite{Algo.MBVE}, some methods use the model as the imagined environment to learn model-free policy in it \cite{Algo.WorldModels}, other methods focus on simulation-based search using the model \cite{Algo.AlphaZero} and there are even methods that integrate model-free and model-based approaches \cite{Algo.I2A}. The model allows the agent to simulate an outcome of an action taken in a given state. The main upside of this is that it allows the agent to plan by thinking ahead, seeing what would happen for a range of possible choices, and explicitly deciding between possible options without the risk of the adverse consequences of trial-and-error in the real environment - including making poor, irreversible decision. Agents can then distill the results from planning ahead into a policy. Even if the model needs to be synthesized from past real experience first it can exploit additional unsupervised learning signals, like rewards function modeling or future observations prediction \cite{Algo.AuxiliaryTasks}, thus it results in a substantial improvement in sample efficiency over model-free methods. Furthermore, the same model can be used by the agent to complete other tasks in the same environment \cite{Algo.I2A}. It gives AI hint of human intelligence flexibility and versatility.

\editnote{* Planning vs. learning differences/similarities...}
Learning is different from planning. In the former the agent samples episodes of real experience through interaction with an environment and updates its policy or states' value estimates based on them. In the latter the agent also updates its policy or states' value estimates, but this time based on simulated experience gained through evaluation of a model. It is worth noting the symmetry which yields one important implication: algorithms for reinforcement learning can also become algorithms for planning, simply by substituting simulated experience in place of real experience. Moreover, planning carries the promise of increasing performance just by increasing the computational budget for searching for good actions \cite{Algo.AlphaGoZero}. Model-free methods to improve their performance need more interactions with a real environment and hence scale in amount of data not amount of computation, which very often is much cheaper than collecting more data.

\editnote{* Really briefly about what is problem of learning world dynamics of POMDP...}
Model-free methods are more popular and have been more extensively developed and tested than model-based methods. While model-free methods forego the potential gains in sample efficiency from using a model, they tend to be easier to implement and tune. The main downside of model-based reinforcement learning is that a ground-truth model of the environment is usually not available to an agent. If the agent wants to use a model in this case, it has to learn the model from experience, which creates several challenges. One of them is bias in the model that can be exploited by the agent \cite{Algo.WorldModels}, resulting in an agent which performs well with respect to the learned model, but behaves sub-optimally in the real environment. Different challenge also comes from fundamental downside of function approximation, it result in models that are inherently imperfect. The performance of agents employing common planning methods usually suffer from seemingly minor model errors \cite{Study.ModelErrorinMC}. Those errors compound during planning, causing more and more inaccurate predictions the further horizon of a plan. 

\editnote{* Long-term ambitious goal...}
There are many real-world problems that could benefit from application of general planning AI system. Company called DeepMind, driven by their experience from creating winning Go search algorithm AlphaZero \cite{Algo.AlphaZero}, published AlphaFold \cite{Algo.AlphaFold}, a system that predicts protein structure. The 3D models of proteins that AlphaFold generates are far more accurate than any that have come before making significant progress on one of the core challenges in biology. Real-world applications of AI algorithms like this are often limited by the problem of sample inefficiency. In a setting with e.g. a physical robot the AI agent can not afford much trial-and-error behaviour, that could cause damage to the robot, and do this over hundreds of millions of time steps, for each task separately, in order to build a sufficiently large training dataset. Those machines work in the real world, not accelerated computer simulation, and often need a human assistance. To apply sample efficient model-based systems, that can generalize their knowledge, accurate learned models and robust planning algorithms are needed.

\editnote{* Explain your topic...}
The aim of this work is to derive from previous work on model-learning in complex high-dimensional decision making problems \cite{Algo.RecurrentEnvSim}\cite{Algo.JointFrameRewardPrediction}\cite{Algo.FastGenerativeModels}\cite{Algo.PlaNet} and apply them to plan in Atari 2600 games, a platform for evaluating general competency in artificial intelligence \cite{Benchmark.RevisitingALE}. Those methods proved to train accurate models, at least in short horizon, and should open a path for application of simulation-based search planning algorithms like TD-Search \cite{Algo.TDSearch} or AlphaZero \cite{Algo.AlphaZero} to complex planning problems in environments with discrete state-action spaces and without access to a ground-truth model. This should allow for improvement in data efficiency and generalization \editnote{We don't test it, maybe it should be omitted then?} without loss in performance. This work focuses on three benchmarks: an arcade game with dense rewards Boxing, a challenging environment with sparse rewards Freeway and a complex puzzle environment MsPacman.
