\section{Introduction}

\editnote{* What is RL and model-free RL...}
Reinforcement learning, a subfield of artificial intelligence (AI), formalise rather the most obvious and common learning strategy among animals. It is learning how to achieve predefined goals through interaction with an environment \cite{Book.RLAI}. Progress has been made in developing capable agents for numerous domains using deep neural networks in conjunction with model-free reinforcement learning \cite{Algo.Rainbow}\cite{Algo.A3C}\cite{Algo.PPO}, where raw observations directly map to agent's actions. However, current state-of-the-art approaches are very sample inefficient, they sometimes require tens or even hundreds of millions of interactions with the environment \cite{Benchmark.RevisitingALE}, and lack the behavioural flexibility of human intelligence, hence the resulting policies poorly generalize to novel tasks in the same environment.

\editnote{* Model-based RL with its benefits...}
The other branch of reinforcement learning algorithms, called model-based reinforcement learning, aims to address these shortcomings by endowing agents with a model of its environment. There are many ways of using the model: it can be used for data augmentation for model-free methods \cite{Algo.MBVE}, some methods use the model to simulate experience for model-free methods to learn from \cite{Algo.WorldModels}, other methods focus on simulation-based search using the model \cite{Algo.AlphaZero} and there are even methods that integrate model-free and model-based approaches into the one architecture \cite{Algo.I2A}. The model allows the agent to simulate an outcome of an action taken in a given state. The main upside is, that it allows the agent to plan by thinking ahead, seeing what would happen for a range of possible choices, and explicitly deciding between possible options without the risk of the adverse consequences of trial-and-error in the real environment - including making poor, irreversible decision. Even if the model needs to be synthesized from past real experience, it can be done more sample-efficiently than model-free methods, because it does not require propagating rewards through Bellman backups and can exploit additional unsupervised learning signals like future observations. Furthermore, the same model can be used by the agent to complete other tasks in the same environment \cite{Algo.I2A} and planning carries the promise of increasing performance just by increasing the computational budget for searching for actions \cite{Algo.AlphaGoZero}. It gives AI hint of human intelligence flexibility and versatility.

\editnote{* Really briefly about what is problem of learning world dynamics of POMDP...}
Model-free methods are more popular and have been more extensively developed and tested than model-based methods. While model-free methods forego the potential gains in sample efficiency from using a model, they tend to be easier to implement and tune. The main downside of model-based reinforcement learning is that a ground-truth model of the environment is usually not available to an agent. If the agent wants to use a model in this case, it has to learn the model from experience, which creates several challenges. Because the model is often only an approximation of real environment, one of them is bias in the model that can be exploited by the agent \cite{Algo.WorldModels}, resulting in an agent which performs well with respect to the model, but behaves sub-optimally in the real environment. Different challenge also comes from fundamental downside of function approximation. The performance of agents employing common planning methods usually suffer from seemingly minor model errors  \cite{Study.PlanWithImperfectModel}. Those errors compound during planning, causing more and more inaccurate predictions the further horizon of a plan \cite{Study.CompoundingModelError}.

\editnote{* Long-term ambitious goal...}
There are many real-world problems that could benefit from application of general planning AI system. Company called DeepMind, driven by their experience from creating winning Go search algorithm AlphaZero \cite{Algo.AlphaZero}, published AlphaFold \cite{Algo.AlphaFold}, a system that predicts protein structure. The 3D models of proteins that AlphaFold generates are far more accurate than any that have come before making significant progress on one of the core challenges in biology. \\
Real-world applications of AI algorithms like this are often limited by the problem of sample inefficiency. In a setting with e.g. a physical robot the AI agent can not afford much trial-and-error behaviour, that could cause damage to the robot. It also can not afford running for hundreds of millions of time steps, for each task separately, in order to build a sufficiently large training dataset. Those machines work in the real world, not in an accelerated and parallelised computer simulation, and often need a human assistance. Progress in sample-efficient model-based algorithms is required in order to bring reinforcement learning into the real world applications.

\editnote{* Explain your topic...}
The aim of this work is to derive from previous work on model-learning in complex high-dimensional decision making problems \cite{Algo.RecurrentEnvSim}\cite{Algo.JointFrameRewardPrediction}\cite{Algo.FastGenerativeModels}\cite{Algo.PlaNet} and apply them to planning in complex tasks. Those methods proved to train accurate models, at least in short horizon, and should open a path for application of planning algorithms like AlphaZero \cite{Algo.AlphaZero} to i.e. Atari 2600 games, a platform used for evaluation of general competency in artificial intelligence \cite{Benchmark.RevisitingALE}. It should allow for improvement in data efficiency without loss in performance compared to model-free methods. This work focuses on three benchmarks: an arcade game with dense rewards Boxing, a challenging environment with sparse rewards Freeway and a complex puzzle game Sokoban.
