\section{State of the art}

\subsection{World Models}

\editnote{High-level idea or abstract.} In World Models \cite{Algo.WorldModels} paper, its authors explore the idea of using large and highly expressive neural networks, that can learn rich spatial and temporal representation of data, and applying them to reinforcement learning. The RL algorithm is often bottlenecked by the credit assignment problem, which makes it hard for traditional RL algorithms to learn millions of weights of a large model. To accomplish their goal, they decompose the problem of an agent training into two stages: they first train a generative neural network to learn a model of the agent's world in an unsupervised manner. Thereafter, by using a compressed spatial and temporal representation of the environment extracted from the world model as inputs to the agent, they train a linear model to learn to perform a task in the environment. The small linear model lets the training algorithm focus on the credit assignment problem on a small search space, while not scarifying capacity and expressiveness via the larger world model.

Their solution consists of three components: Vision for encoding the spatial information, Memory for encoding the temporal information and Controller which represents the agent's policy. Fig.~\ref{Fig.WorldModels} depicts a flow diagram of the agent's model.

\begin{figure}[H]
\includegraphics[width=0.7\textwidth,keepaspectratio]{figures/WorldModels.png}
\caption[Flow diagram of the World Models agent's model]{Flow diagram of the agent's model \protect\cite{Algo.WorldModels}. The raw observation is first processed by the Vision at each time step $t$ to produce $z_t$. The input into the Controller is this latent vector $z_t$ concatenated with the Memory hidden state $h_t$ at each time step. The Controller will then output an action vector $a_t$ and will affect the environment. The Memory will then take the current $z_t$ and action $a_t$ as an input to update its own hidden state to produce $h_{t+1}$ to be used at time step $t + 1$.}
\label{Fig.WorldModels}
\end{figure}

The environment provides the agent with high dimensional visual observation at each time step. The essential task of the Vision model is to encode this high dimensional observation into a low dimensional latent state. To do this, Vision is implemented as Variational Autoencoder \cite{Algo.VAE}. It is trained in an unsupervised manner on randomly generated experience from the environment. The authors assume that the random agent can efficiently explore environment and no iterative training procedure is implemented. The dataset is gathered once and fixed for Vision and Memory training.

Since many complex environment are partially observable, the visual observation at each time step, and hence the latent state, doesn't include full information about the current situation in the environment. To acquire full knowledge, the agent needs to encode what happens over time. This is the role of the Memory. It is implemented as popular recurrent neural network (RNN) architecture called Long Short-Term Memory \cite{Algo.LSTM} and trained on the same data as Vision to predict the next step future latent state that Vision is expected to produce. Because many environments are stochastic in nature, the RNN is trained to output a probability density of the next latent state approximated as a mixture of Gaussian distribution - in literature, this approach is known as Mixture Density Network combined with a RNN \cite{Algo.MDNRNN} (MDN-RNN). Moreover, using the stochastic Memory the authors are able to train more robust Controller, more on that later. \\
To be more precise, the MDN-RNN will model $p(z_{t+1} | o_{\leqslant t}, a_{\leqslant t}) = p(z_{t+1} | h_t) \prod_{i=1}^t q(z_i | o_i)$, where $h_t = f(h_{t-1}, z_t, a_t)$ is the hidden state of the RNN, $f$, that encodes past information about states and actions from the beginning of the episode until the time step $t$. Furthermore, $o_{t}$, $z_{t}$ and $a_{t}$ are the observation, the latent state and the action at time step $t$ respectively. During sampling they can adjust a temperature parameter $\tau$, that scale mixing coefficients in the MDN, to control model uncertainty \cite{Algo.Sketch-RNN}. They find it useful for training the Controller later on.

The Controller model represents the agent's policy. It is responsible for determining course of actions to take in order to solve a given task. Controller is a simple linear model that maps the concatenated latent state $z_t$ and hidden state $h_t$ at the time step $t$ directly to the action $a_t$ at that time step: $a_t = W[z_t h_t] + b$, where $W$ and $b$ are the weight matrix and bias vector of that model.
The authors deliberately made Controller as simple as possible, and trained it separately from Vision and Memory, so that most of the agent's complexity resides in the world model (V and M). The latter can take the advantage of current advances in deep learning that provide tools to train large models efficiently when well-behaved and differentiable loss function can be defined.
Shift in the agent's complexity towards the world model allows the Controller model to stay small and focus its training on tackling the credit assignment problem in challenging RL tasks. It is trained using evolution strategy, which is rather an unconventional choice that only currently have been considered as a viable alternative to popular RL techniques \cite{Algo.ESRL}.

\editnote{Main contributions or where it's SoTa.} Their solution is able to solve an OpenAI Gym's CarRacing environment \cite{Code.OpenAIGym}, which is the continuous-control, top-down racing task. It is the first known solution to achieve the score required to solve this task. In the process, the Memory model learns to simulate the original environment. The authors show that the learned Controller can function inside of the imagined environment of CarRacing, that is simulated by the Memory.
In the second experiment, they show that the agent is able to learn solely from imagined experience, produced by its Memory, and successfully transfer this policy back to the actual environment of VizDoom (see fig.~\ref{Fig.VizDoom}). In the first trials, the Controller learned to exploit imperfect simulations of the Model, which only approximates the true environment dynamics. To mitigate this behaviour, they adjust the temperature parameter of MDN-RNN to control the amount of randomness in the Memory, hence controlling the trade-off between realism and exploitability.

\begin{figure}[H]
\includegraphics[width=0.7\textwidth,keepaspectratio]{figures/VizDoom.png}
\caption[VizDoom]{VizDoom: the agent must learn to avoid fireballs shot by monsters from the other side of the room with the sole intent of killing the agent \protect\cite{Algo.WorldModels}.}
\label{Fig.VizDoom}
\end{figure}

\editnote{Argument why and how (or not) you base on this work.} The authors results indicate that their world model is able to model complex environments from visual observations and it can be used for planning. Therefore, it may prove useful for the topic of this thesis.

\subsection{Learning Latent Dynamics for Planning from Pixels}

\editnote{High-level idea or abstract.}

\editnote{Main contributions or where it's SoTa.}

\editnote{Argument why and how (or not) you base on this work.}
