\section{State of the art}

Quite surprisingly, there is not much work on model-based planning and learning from high-dimensional observations, like images, in complex environments, like video games. This chapter review related work that helps arrive at the final solution of this thesis problem of planning in imagination.

Planning is a natural and powerful approach to decision making problems with known dynamics. For instance, Monte-Carlo Tree Search methods \cite{Algo.MCTS} have been used for complex search problems, such as the game of Go \cite{Algo.AlphaGoZero}. Moreover, planning carries the promise of increasing performance just by increasing the computational budget for searching for actions. The first paper below describes the current state of the art search method and the other one explores the idea of Monte-Carlo planning with a learned model.

\subsection{Learning and querying world models} \label{Sec.FastWorldModels}

In \cite{Algo.FastGenerativeModels}, the authors aim to address the challenge of learning accurate, computationally efficient models of complex domains, a key challenge in model-based reinforcement learning, and using them to solve RL problems. First, they advocate the use of computationally efficient state-space environment models that make predictions at a higher level of abstraction, both spatially and temporally, than at the level of raw pixel observations. Such models substantially reduce the amount of computation required to make predictions, as future states can be represented much more compactly. Second, in order to increase model accuracy, they examine the benefits of explicitly modeling uncertainty in state transitions.

In the fig.~\ref{Fig.FastGenerativeModels}, there are different environment models described in the paper. The models, $p$, are learned in an unsupervised way from observations, $o$, conditioned on actions, $a$. In particular, the authors focus on how fast and accurately models can predict, at time step $t$, some future statistics $x_{t+1:t+\tau}$, e.g. an environment's rewards, over a horizon $\tau$ by simulating a trajectory given an arbitrary sequence of actions $a_{t:t+\tau−1}$, that can later be used for decision making.

\begin{figure}[H]
\includegraphics[width=0.9\textwidth,keepaspectratio]{figures/FastGenerativeModels.png}
\caption[Fast Generative Models]{The graphical models representing the architectures of different environment models. Boxes are deterministic nodes, circles are random variables and filled circles represent variables observed during training \protect\cite{Algo.FastGenerativeModels}.}
\label{Fig.FastGenerativeModels}
\end{figure}

A straight-forward choice is the family of temporally auto-regressive models (AR) over the observations, $o_{t+1:t+\tau}$. If these use a recurrent mapping that recursively updates sufficient statistics $s_t = f(s_{t−1}, a_{t−1}, o_{t−1})$, therefore reusing the previously computed statistics $s_{t-1}$, and use them to predict future observations, $p(o_{t+1:t+\tau} | o_{\leqslant t}, a_{<t+\tau}) = \prod^{t+\tau}_{r=t+1} p(o_r | f(s_{r−1}, a_{r−1}, o_{r−1}))$, then the authors call these models recurrent auto-regressive models (RAR). If $f$ is parameterised as a neural network, RARs are equivalent to recurrent neural networks. Although faster then auto-regressive models that don't reuse statistics from previous time steps, these are still expect to be slow when simulating trajectories, as they still need to explicitly render observations $o_{t+1:t+\tau}$ in order to make any predictions, $x_{t+1:t+\tau}$.

State-space models (SSMs) circumvent this by positing that there is a compact state representation $s_t$ that captures all essential aspects of the environment on an abstract level. It is assumed that $s_{t+1}$ can be predicted from the previous state $s_t$ and action $a_t$ alone, without the help of previous pixels $o_{\leqslant t}$, $p(s_{t+1}|s_{\leqslant t}, a_{<t}, o_{\leqslant t}) = p(s_{t+1}| s_t, a_t)$. Furthermore, it is assumed that $s_t$ is sufficient to predict $o_t$, i.e. $p(o_t|s_{\leqslant t}, a_{<t}) = p(o_t|s_t)$. This modelling choice implies that the latent states are, by construction, sufficient to generate any future predictions $x_{t+1:t+\tau}$. Hence, the model never has to directly sample costly high-dimensional pixel observations.

The authors consider two flavors of SSMs: deterministic SSMs (dSSMs) and stochastic SSMs (sSSMs). For dSSMs, the latent transition $s_{t+1} = f(s_t, a_t)$ is a deterministic function of the past, whereas for sSSMs, they consider transition distributions $p(s_{t+1}|s_t, a_t)$ that explicitly model uncertainty over the next state. The sSSMs are parameterised by introducing for every $t$ a latent variable $z_t$ whose distribution depends on $s_{t−1}$ and $a_{t−1}$, and by making the state a deterministic function of the past state, action, and latent variable: $z_{t+1} \sim p(z_{t+1}|s_t, a_t)$, $s_{t+1} = f(s_t, a_t, z_{t+1})$.

The observation model, also called decoder, computes the conditional distribution $p(o_t|\cdot)$. It either takes as input the state $s_t$ (deterministic decoder), or the state $s_t$ and latent $z_t$ (stochastic decoder). sSSMs always use the stochastic decoder. dSSMs can use either the deterministic decoder (dSSM-DET), or the stochastic decoder (dSSM-VAE). The latter can capture joint uncertainty over pixels in a given observation $o_t$, but not across time steps. The former is a fully deterministic model, incapable of modeling joint uncertainties (in time or in space).

The paper provides the first comparison of deterministic and stochastic, pixel-space and state-space models w.r.t. speed and accuracy, applied to challenging environments from the Arcade Learning Environment \cite{Code.ALE}. Specifically, the prediction of dSSM-DET exhibits “sprite splitting”, or layering of multiple possible future observations, at corridors of MS-PACMAN \cite{Game.MSPACMAN}, whereas multiple samples from the sSSM show that the model has a reasonable and consistent representation of uncertainty in this situation. Moreover, the authors note that SSMs, which avoid computing pixel renderings at each rollout step, exhibit a speedup of more then 5 times over the standard AR model.

Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from raw pixels. The computational speed-up of state-space models, while maintaining high accuracy, makes their application in RL feasible. The authors demonstrate that agents which query these models for decision making outperform strong model-free baselines in MS-PACMAN game, demonstrating the potential of using learned environment models for planning.

The authors explore the topic of learning fast generative models that can be used in model-based reinforcement learning for planning. Their ideas find application in PlaNet \cite{Algo.PlaNet} and World Models \cite{Algo.WorldModels} papers, which use the stochastic state-space model variation, and hence in this thesis solution too.

\subsection{World Models}

In World Models \cite{Algo.WorldModels} paper, its authors explore the idea of using large and highly expressive neural networks, that can learn rich spatial and temporal representation of data, and applying them to reinforcement learning. The RL algorithm is often bottlenecked by the credit assignment problem, which makes it hard for traditional RL algorithms to learn millions of weights of a large model. To accomplish their goal, they decompose the problem of agent training into two stages: they first train a generative neural network to learn a model of the agent's environment, in an unsupervised manner. Thereafter, by using a compressed spatial and temporal representation of the environment extracted from the world model as inputs to the agent, they train a linear model to perform a task in the environment. The small linear model lets the training algorithm focus on the credit assignment problem on a small search space, while not sacrificing capacity and expressiveness via the larger world model.

Their solution consists of three components: Vision for encoding the spatial information, Memory for encoding the temporal information and Controller which represents the agent's policy. Fig.~\ref{Fig.WorldModels} depicts a flow diagram of the agent's model.

\begin{figure}[H]
\includegraphics[width=0.7\textwidth,keepaspectratio]{figures/WorldModels.png}
\caption[Flow diagram of the World Models agent's model]{Flow diagram of the agent's model \protect\cite{Algo.WorldModels}. The raw observation is first processed by the Vision (V) at each time step $t$ to produce a latent variable $z_t$. The input into the Controller (C) is the latent variable $z_t$ concatenated with the Memory (M) hidden state $s_t$ at each time step. The Controller will then output an action vector $a_t$ and will affect the environment and produce the next observation. The Memory will then take the current $z_t$ and action $a_t$ as an input to update its own hidden state to produce $s_{t+1}$ to be used at time step $t + 1$.}
\label{Fig.WorldModels}
\end{figure}

The environment provides the agent with a high-dimensional visual observation, a game frame, at each time step. The essential task of the Vision module is to encode this high dimensional observation, $o$, into a low dimensional latent variable, $z$, using an encoder, $q(z|o)$. It is trained in an unsupervised manner to reproduce the original game frame passed as input to the encoder and then decoded by its decoder, $p(o|z)$. It uses randomly generated experience from the environment as the dataset. The authors assume that the random agent can efficiently explore environment.

Since many complex environment are partially observable, the visual observation at each time step, and hence the latent variable, does not include full information about the current situation in the environment. To acquire full knowledge, the agent needs to encode what happens over time. This is the role of the Memory module. Because many environments are stochastic in nature, Memory is trained to predict a probability density of the next latent variable. Moreover, using the stochastic Memory the authors are able to train more robust Controller. During sampling the authors adjust a temperature parameter $\tau$ to control model uncertainty \cite{Algo.Sketch-RNN}. They find it useful for training the Controller later on.

This architecture, although quite changed, still reassembles sSSM from section \ref{Sec.FastWorldModels}. It learns the Memory module in the latent space created by the Vision module. It also explicitly models transitions uncertainty, which will prove to be the key for successful Controller training. What is different though, it predicts the next latent variable conditioned on the next hidden state, $z_{t+1} \sim p(z_{t+1}|s_{t+1})$. The Memory module calculates its next hidden state based on the present hidden state, action and latent variable, $s_{t+1} = f(s_t, a_t, z_t)$. This way, the Memory module is less a sample transition model that predicts one future from all possible futures and more a memory which deterministically encodes facts about the past in the hidden state, which should fully describe the present latent state of the environment, and then use it to sample a latent variable.

The Controller model represents the agent's policy. It is responsible for determining course of actions to take in order to solve a given task. Controller is a simple linear model that maps the concatenated latent variable $z_t$ and hidden state $s_t$ at the time step $t$ directly to the action $a_t$ at that time step.

The authors deliberately made Controller as simple as possible, and trained it separately from Vision and Memory, so that most of the agent's complexity resides in the world model (Vision and Memory). The latter can take the advantage of current advances in deep learning that provide tools to train large models efficiently when well-behaved and differentiable loss function can be defined.
This shift in the agent's complexity towards the world model, as already mentioned, allows the Controller model to stay small and focus its training on tackling the credit assignment problem in challenging RL tasks. It is trained using evolution strategy, which is rather an unconventional choice that only currently have been considered as a viable alternative to popular RL techniques \cite{Algo.ESRL}.

Their solution is able to solve an OpenAI Gym's CarRacing environment \cite{Code.OpenAIGym}, which is the continuous-control racing task, with top-down view on the car and the track. It is the first known solution to achieve the score required to solve this task. Nonetheless, because the Controller uses real experience for training counted in millions of episodes, they did not improve sample-efficiency compared to other model-free solutions \cite{Algo.CarRacingA3C}. But, what is really interesting, in the process of training the Memory learns to simulate the original environment. The authors show that the learned Controller can function inside of the imagined environment of CarRacing, that is, simulated by the Memory.
In the second experiment, they show that the agent can not only play in imagination, but also it is able to learn solely from imagined experience, produced by its Memory, and successfully transfer this policy back to the actual environment of VizDoom (see fig.~\ref{Fig.VizDoom}). In the first trials, the Controller learned to exploit imperfect simulations of the Model, which only approximates the true environment dynamics. To mitigate this behaviour, they adjust the temperature parameter of Memory to control the amount of randomness in the simulation, hence controlling the trade-off between realism and exploitability.
The Controller learns from simulated experience, which means that only tens of thousands of episodes from the environment are needed for the training of Vision and Memory. Assuming that each episode consists of hundreds of frames, millions of frames in total are required for training. It makes World Models very sample efficient compared to other state-of-the-art model-free methods that require even two orders of magnitude more data \cite{Algo.A3C}.

\begin{figure}[H]
\includegraphics[width=0.7\textwidth,keepaspectratio]{figures/VizDoom.png}
\caption[VizDoom]{VizDoom: the agent must learn to avoid fireballs shot by monsters from the other side of the room with the sole intent of killing the agent \protect\cite{Algo.WorldModels}.}
\label{Fig.VizDoom}
\end{figure}

The authors results indicate that their world model is able to model complex environments from visual observations and it can be used for planning. Therefore, it may prove useful for the topic of this thesis.

\subsection{Learning Latent Dynamics for Planning from Pixels}

In \cite{Algo.PlaNet}, the authors propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. This approach uses a latent dynamics model for its fast querying capabilities, described earlier in section \ref{Sec.FastWorldModels}. Moreover, they propose a multi-step variational inference objective named latent overshooting already described in section \ref{Sec.ModelLearning}. The authors found that several dynamics models benefit from latent overshooting, although their final agent, which use the RSSM model described below, does not require it.

Despite its generality, the purely stochastic transitions make it difficult for the transition model to reliably remember information for multiple time steps. In theory, this model could learn to set the variance to zero for some state components, but the optimization procedure may not find this solution. This motivates including a deterministic sequence of activation vectors $\{s_t\}^T_{t=1}$ that allow the model to access not just the last latent variable, $z_{t-1}$, but all previous states deterministically. The authors use such a model, shown in fig.~\ref{Fig.PlaNetModelDesigne}, that they name recurrent state-space model (RSSM). Intuitively, one can understand this model as splitting the state into a stochastic part $z_t$ and a deterministic part $s_t$, which depend on the stochastic and deterministic parts at the previous time step.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth,keepaspectratio]{figures/PlaNet/model.png}
\caption[PlaNet latent dynamics model design]{Latent dynamics model design \protect\cite{Algo.PlaNet}. In this example, the model observes the first two time steps and predicts the third. Circles represent stochastic variables and squares deterministic variables. Solid lines denote the generative process and dashed lines the inference model. This model splits the latent state into stochastic and deterministic parts, allowing the model to robustly learn to predict multiple futures.}
\label{Fig.PlaNetModelDesigne}
\end{figure}

Similar to sSSM from section \ref{Sec.FastWorldModels}, it learns a transition model in latent space for efficiency and it explicitly models transitions uncertainty for accuracy. Like in World Models, it condition a latent variable only on a deterministic state. It calculates the next deterministic state based on the present deterministic state, action and latent variable, $s_{t+1} = f(s_t, a_t, z_t)$ and samples the next latent variable based on the next state, $z_{t+1} \sim p(z_{t+1}|s_{t+1})$. It can be viewed that stochastic part of the state is determined by deterministic part of the state at any time step. This is reversed relationship relative to sSSM and it helps preserve a deterministic part of the state for multiple time steps and this way further improve the simulation accuracy. \\
RSSM also learns an encoder $q(z_t | o_{\leqslant t}, a_{< t})$, observation model $p(o_t | s_t, z_t)$, also called a decoder, and reward model $p(r_t | s_t, z_t)$. The encoder is used to infer an approximate belief over the current latent variable from the history. The observation model provides a rich training signal but is not used for planning.

Given these components, the authors implement the policy as a planning algorithm that searches for the best sequence of future actions. In contrast to model-free and hybrid reinforcement learning algorithms, the authors do not use a policy or value network. \\
The cross entropy method \cite{Algo.CEM} (CEM) is used to search for the best action sequence under the model. The authors decided on this algorithm because of its robustness and because it solved all considered tasks when given the true dynamics for planning. CEM is a population-based optimization algorithm that infers a distribution over action sequences that maximize the sum of rewards for a short future horizon. It uses the model to evaluate sampled candidate sequences in the optimization process. Because the reward is modeled as a function of the latent state, it can operate purely in latent space without generating images, which allows for fast evaluation of large batches of action sequences. After optimization finished, the first action from the resulting sequence is issued to the environment and the process starts from scratch in the next state.

Since the agent may not initially visit all parts of the environment, it iteratively collect new experience and further refine the dynamics model.

Using only pixel observations, PlaNet's agent solves continuous control tasks from DeepMind control suite (see fig.~\ref{Fig.DeepMindControlSuite}) with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.

\begin{figure}[H]
\includegraphics[width=1.0\textwidth,keepaspectratio]{figures/PlaNet/benchmarks.png}
\caption[DeepMind Control Suite]{DeepMind Control Suite: image-based control domains used in PlaNet's experiments \protect\cite{Algo.PlaNet}.}
\label{Fig.DeepMindControlSuite}
\end{figure}

Within 100 episodes, PlaNet outperforms the policy-gradient method A3C \cite{Algo.A3C} trained from proprioceptive states for 100,000 episodes, on all tasks. After 500 episodes, it achieves performance similar to D4PG \cite{Algo.D4PG}, trained from images for 100,000 episodes, except for the finger task. PlaNet surpasses the final performance of D4PG with a relative improvement of 26\% on the cheetah running task.

Moreover, the authors trained a single agent on all six tasks. The agent is not told which task it is facing, it needs to infer this from the image observations. The agent solves all tasks while learning slower compared to individually trained agents. This indicates that the model can learn to predict multiple domains, regardless of the conceptually different visuals.

PlaNet is a working example of a model-based agent that learns a latent dynamics model from high-dimensional image observations and chooses actions by fast planning in latent space. The authors show that their agent succeeds at several continuous control tasks from image observations, reaching performance that is comparable to the best model-free algorithms while using 200 times fewer episodes and similar or less computation time. The results show that learning latent dynamics models for planning in image domains is a promising approach. This thesis will adopt PlaNet to its objectives.

\subsection{Model-Based Reinforcement Learning for Atari}

In \cite{Algo.SimPLe}, the authors explore how video prediction models can enable RL agents to solve Atari games with orders of magnitude fewer interactions than model-free methods. They describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models, and present a comparison of several model architectures, including a novel architecture that yields the best results in Atari games \cite{Code.ALE}.

SimPLe, apart from an Atari 2600 emulator environment $env$, use a neural network simulated environment $env'$ which they call a world model. The authors find out that crucial decisions in the design of world models are: inclusion of stochasticity, clipping reconstruction loss to a constant and, because simulator $env'$ is supposed to consume its own predictions from previous steps which will be imperfect due to compounding errors, to mitigate a problem of the model drifting out of the area of its applicability they randomly replace in training some frames of the input by the model prediction from the previous step. 

The environment $env'$ shares the action space and reward space with $env$ and produces visual observations in the same format, as it will be trained to mimic $env$. The authors principal aim is to train a policy $\pi$ using a simulated environment $env'$ so that $\pi$ achieves good performance in the original environment $env$. Using short rollouts for a policy training is crucial to mitigate the compounding errors problem described in section \ref{Sec.ModelLearning}. To ensure exploration of further episode states, SimPLe starts training rollouts from randomly selected states taken from the real data buffer.

In this training process the authors aim to use as few interactions with $env$ as possible. The initial data to train $env'$ comes from random rollouts of $env$. As this is unlikely to capture all aspects of the environment, they use the data-aggregation iterative method.

Experiments evaluate SimPLe on a range of Atari games and show that it achieves competitive results compared to model-free baselines with only 100K interactions between the agent and the environment, which corresponds to about two hours of real-time play. SimPLe is significantly more sample-efficient than a highly tuned version of the state-of-the-art Rainbow algorithm \cite{Algo.Rainbow} on almost all games. In particular, in low data regime of 100k samples, on more than half of the games, SimPLe achieves a score which Rainbow requires at least twice as many samples. In the best case of Freeway, it is more than 10x more sample-efficient.

SimPLe is a similar approach to model-based RL like in World Models. The authors train the dynamics model and use it to generate new experience, the same as in World Models, but in observations space, opposite to World Models. \\
Although this thesis share the goal of sample efficient RL via model-based planning and learning in complex environments, like Atari games, with SimPLe, it tries to accomplish it in fundamentally different way. The thesis solution focuses on training accurate transitions model in the latent state-space to enable fast simulation and search using this model. Pixel-perfect reconstructions are not a concern.

\subsection{AlphaZero: a general reinforcement learning algorithm}

AlphaZero \cite{Algo.AlphaZero} replaces the handcrafted knowledge and domain-specific augmentations used in traditional game-playing programs with deep neural networks, a general-purpose reinforcement learning algorithm, and a general-purpose tree search algorithm. Instead of handcrafted move ordering and evaluation function heuristics, AlphaZero infers policy and state value with a deep neural network. This neural network takes the board position as an input and outputs a vector of move probabilities for each action and a scalar value estimating the expected outcome of the game. AlphaZero learns these move probabilities and value estimates entirely from self-play. These are then used to guide its search in future games.

Instead of an alpha-beta search with domain-specific enhancements, as common in board games playing programs, AlphaZero uses a general-purpose Monte Carlo tree search \cite{Algo.MCTS} algorithm. Each search consists of a series of simulated games that traverse a tree from a root state until a leaf state is reached. Each simulation proceeds by selecting in each state a move with low visit count (not previously frequently explored), high move probability and high value (averaged over the leaf states of simulations that selected this move previously) according to the current neural network. The search returns a vector representing a probability distribution over moves from the root state. This algorithm and its training is described in more depth in the chapter \ref{Sec.SolutionDescription}.

Policy and value networks can be though of as intuition build upon experience. The longer the algorithm play with the environment it found itself in, the better it has intuition about good moves and how good are particular states. Because it uses deep neural network, its intuition can generalise to unseen states. It is very important in environments where state-space is so big, that it can not be explored exhaustively, like the game of Go \cite{Algo.AlphaGoZero}. It then uses this intuition for stronger play. The most important thing is, though, AlphaZero does not exploit any domain knowledge in its architecture. It is built from general building blocks, which then learn the task at hand.

Starting from random play, AlphaZero convincingly defeats a world champion programs in the games of chess and shogi as well as Go. Especially, the game of chess represents the pinnacle of artificial intelligence research over several decades. State-of-the-art programs are based on powerful engines that search many millions of positions, leveraging handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic reinforcement learning and search algorithm, originally devised for the game of Go, that achieve superior results within a few hours, searching 1/1000 as many positions and it is given no domain knowledge except the rules of chess. Furthermore, the same algorithm was applied without modification to the more challenging game of shogi, again outperforming state-of-the-art programs within a few hours.

AlphaZero is the state-of-the-art general search algorithm, but it requires a dynamics model of an environment in order to work. The model is not always available, but could be learned in such cases using methods described before. This thesis tries to enable AlphaZero to work with the learned model.

\subsection{Value Prediction Network}

Paper \cite{Algo.VPN} proposes a novel deep reinforcement learning architecture, called Value Prediction Network (VPN), which integrates model-based planning and model-free learning of reward and value functions into a single neural network. In contrast to typical model-based RL methods, VPN learns the dynamics model of an abstract state space sufficient for predicting future rewards and values, rather than future observations. \\
In order to train VPN, the authors propose a combination of temporal-difference search \cite{Algo.TDSearch} and n-step Q-learning \cite{Algo.A3C}. In brief, VPN learns to predict values via Q-learning and rewards via supervised learning. At the same time, VPN perform lookahead planning to choose actions during play and compute target Q-values during training.

VPN has the ability to simulate the future and plan based on the simulated future abstract-states. Although many existing planning methods (e.g. MCTS) can be applied to the VPN, the authors implement a simple planning method which rollout trajectories using the VPN up to a certain depth and aggregates all intermediate value estimates as described in the paper \cite{Algo.VPN}. The planning procedure estimates the current state Q-values which an agent uses to decide on the next action.

Experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic navigation task where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network \cite{Algo.DQN}, strong model-free baseline method, on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.

VPN uses an abstract model of rewards to augment model-free learning with good results on a number of Atari games. However, this method does not actually aim to model or predict future environment's states and achieves clear but relatively modest gains in sample-efficiency. On the other hand, it is an example of simple tree search algorithm which can successfully use the learned model for planning, something that this thesis could base on.
