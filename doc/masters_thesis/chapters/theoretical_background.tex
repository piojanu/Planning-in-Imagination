\section{Theoretical background}

\subsection{Partially Observable Markov Decision Processes}

Markov Decision Processes (MDPs) are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations (states) and through those future rewards. Therefore, MDPs involve delayed rewards and the need to trade-off these with an immediate reward.
Partially observable Markov Decision Processes (POMDPs), which describe a more general class of problems, have one major difference, the full state of an environment is unknown. The environment is perceived through observations that provide only partial information about the state. A good example are Atari games. Individual frames often doesn't provide full information about the game's state which is held in the game's RAM.

In this work following definition of POMDP is used: it consists of a set of hidden states $S$, a set of observations $O$ and a set of actions $A$. The dynamics of the MDP, from any state $s \in S$ and for any action $a \in A$, are determined by transition function, $P^a_{ss'} = p(S_{t+1} = s' | S_t = s, A_t = a)$, specifying the distribution over the next state $s' \in S$. A reward function, $R^a_{ss'} = p(R_{t+1} | S_t = s, A_t = a, S_{t+1} = s')$, specifies the distribution over rewards for a given state transition. Finally, as mentioned earlier, POMDP is perceived through partial observations specified via probability distribution $P_s = p(O_t | S_t = s)$.
A fixed initial state $s_0$ is assumed. In episodic POMDPs, which this work considers, an environment terminates with probability 1 in one of distinguished terminal states, $s_T \in S$, after finite number of transitions $T$. A return $G_t = \sum^T_{k=t+1}\gamma^{k-t-1}r_k$ is the total reward accumulated in that episode from time $t$ until reaching the terminal state at time $T$. $0 \leqslant \gamma \leqslant 1$ is a discount factor that trade-offs short-term rewards with long-term rewards.
A policy, $\pi(s, a) = p(A_T = a | S_t = s)$, maps a state $s$ to a probability distribution over actions.
A value function, $V_\pi(s) = \EX_\pi[G_t | s_t = s]$, is the expected return from state $s$ when following policy $\pi$ where the expectation is over the distributions of the environment and the policy.
An action value function, $Q_\pi(s, a) = \EX_\pi[G_t | s_t = s, a_t = a]$, is the expected return after selecting action $a$ in state $s$ and then following policy $\pi$ where, again, the expectation is over the distributions of the environment and the policy.
An optimal value function is the unique value function that maximises the value of every state, $V^*(s) = \underset{\pi}{\max}V_\pi(s), \forall_{s \in S}$ and $Q^*(s, a) = \underset{\pi}{\max}Q_\pi(s, a), \forall_{s \in S, a \in A}$. An optimal policy $\pi^*(s, a)$ is a policy that maximises the action value function for every state in the POMDP, $\pi^*(s, a) = \underset{a}{\mathrm{argmax}}Q^*(s, a)$.

POMDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. In reinforcement learning the dynamics, the observations distribution and the reward function are hidden behind an environment. Consequently, we can not directly use them for planning, but we can learn them through interaction with the environment.

\subsection{Reinforcement Learning}

Reinforcement learning (RL) is learning what to do, how to map situations to actions, so as to maximize a numerical reward signal. \cite{Book.RLAI} This mapping is called a policy $\pi$. RL consists of an agent that, in order to learn a good policy, acts in an environment. The environment provides a response to each agent's action $a$ that is interpreted and fed back to the agent. Reward $r$ is used as a reinforcing signal and state $s$ is used to condition agent's decisions. Fig.~\ref{Fig.RL} explains it in the diagram.
Each action-response-interpretation sequence is called a step or a transition. Multiple steps form an episode. The episode finishes in a terminal state $s_T$ and the environment is reset in order to start the next episode from scratch. Very often, RL agents need dozens and dozens of episodes to gather enough experience to learn the (near) optimal policy.

\begin{figure}[H]
\includegraphics[]{figures/RL.jpg}
\caption[Reinforcement Learning]{Reinforcement Learning \protect\cite{Book.RLAI}}
\label{Fig.RL}
\end{figure}

\editnote{TODO: Describe here General Policy Iteration, Monte-Carlo Control, TD-Learning, ... what else? Rather than guessing what needs to be described, continue work and see what needs more attention.}

\subsection{Deep Learning}

Machine learning gives AI systems the ability to acquire their own knowledge, by extracting patterns from raw data. It stands in opposition to classical computer programs which execute explicit instructions hand-coded by a programmer.
One example of machine learning algorithm is logistic regression. It can determine whether to recommend cesarean delivery or not \cite{Study.Cesarean}. Another widely used machine learning algorithm called naive Bayes can distinguish between legitimate and spam e-mail.

The performance of these machine learning algorithms depends heavily on the representation of the problem they are given. For example, when logistic regression is used to recommend cesarean delivery, the AI system does not examine the patient's MRI scan directly. It would not be able to make useful predictions as individual pixels in an MRI scan have negligible correlation with any complications that might occur during delivery. It, instead, gets several pieces of relevant information, such as the presence or absence of a uterine scar, from the doctor. Each piece of information included in the representation of the data is known as a feature. Logistic regression learns the relation between those features and various outcomes, such as a recommendation of cesarean delivery. The algorithm does not influence the way that the features are defined in any way.

Sometimes it can be hard to hand-craft a good problem's representation. For example, suppose that we would like to write a program to detect cats in photographs. We know that cats are furry and have whiskers, so we might like to use the presence of a fur and whiskers as features. Unfortunately, it is difficult to describe exactly what a fur or a whisker looks like in terms of pixel values. This gets even more complicated when we take into account e.g. shadows falling on the cat or an object in the foreground obscuring part of the animal.
One solution to this problem is to use machine learning to discover not only the mapping from representation to output but also the representation itself. This approach is known as representation learning. Learned representations often result in much better performance of machine learning algorithms than can be obtained with hand-designed representations. They also allow AI systems to rapidly adapt to new tasks with minimal human intervention.

Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts, with each concept defined in relation to simpler concepts. Deep learning solves representation learning problem by introducing representations that are expressed in terms of other, simpler representations. Fig.~\ref{Fig.DL} shows how a deep learning system can represent the concept of an image of a person by combining simpler concepts, such as corners and contours, which are in turn defined in terms of edges.

\begin{figure}[H]
\includegraphics[width=0.8\textwidth,keepaspectratio]{figures/DL.png}
\caption[Deep Learning]{Deep Learning \protect\cite{Book.DeepLearning}}
\label{Fig.DL}
\end{figure}

The fundamental example of a deep learning model is a feedforward deep network or multilayer perceptron (MLP). A multilayer perceptron is just a mathematical function mapping some set of input values to output values. The function is formed by composing many simpler functions, called perceptrons, each providing a new representation of its input to next functions. Fig.~\ref{Fig.MLP} shows example MLP and dependencies between perceptrons. The input is presented at the input layer. Then a hidden layer (or series of them) extracts increasingly abstract features from the image. These layers are called “hidden” because their values are not given in the data. Instead, the model must determine which concepts are useful for explaining the relationships in the observed data. Finally, this description of the input in terms of the features can be used to produce the output at the output layer.

\begin{figure}[H]
\includegraphics[width=0.8\textwidth,keepaspectratio]{figures/MLP.png}
\caption{Multilayer perceptron}
\label{Fig.MLP}
\end{figure}

\editnote{TODO: One word about RNNs, Conv. and other used layers here.}
\editnote{TODO: Also write here about backprop.}

\subsection{Variational Inference}
\editnote{TODO: Describe VAEs in structured POMDPs and what are zero step, one step and open loop predicitions.}
