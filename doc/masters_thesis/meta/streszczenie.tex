\section*{Streszczenie}
\subsection*{Metody planowania w głębokim uczeniu ze wzmocnieniem}

Uczenie ze wzmocnieniem, dziedzina sztucznej inteligencji, formalizuje najbardziej oczywistą strategię uczenia się wśród zwierząt. Zwierzęta uczą się jak osiągnąć określone cele poprzez interakcję ze środowiskiem \cite{Book.RLAI}. W ostatnich latach poczyniono znaczne postępy w opracowywaniu agentów zdolnych rozwiązywać problemy z wielu domen przy użyciu głębokich sieci neuronowych w połączeniu z uczeniem ze wzmocnieniem bez modelowania \cite{Algo.Rainbow}\cite{Algo.A3C}\cite{Algo.PPO}. W metodach tych agent w sposób, można by to ująć, odruchowy reaguje na obserwowane stany środowiska. Obecne najnowocześniejsze metody są jednak bardzo nieefektywne ze względu na ilość danych, co oznacza, że ​​czasami wymagają dziesiątek, a nawet setek milionów interakcji ze środowiskiem \cite{Benchmark.RevisitingALE}. Ponadto brakuje im elastyczności ludzkiej inteligencji, stąd wyuczone zachowania słabo przenoszą się na nowe zadania w tym samym środowisku.

Inna gałąź algorytmów uczenia ze wzmocnieniem, zwana uczeniem ze wzmocnieniem opartym na modelowaniu świata, ma na celu rozwiązanie tych niedociągnięć poprzez wyposażenie agentów w model środowiska. Istnieje wiele sposobów korzystania z modelu: można go wykorzystać do augmentacji danych dla metod bez modelowania \cite{Algo.MBVE}, niektóre metody wykorzystują model do symulacji doświadczeń i następnie uczenia się na ich podstawie \cite{Algo.WorldModels}, inne metody koncentrują się na wyszukiwaniu opartym na symulacji przy użyciu modelu \cite{Algo.AlphaZero} oraz istnieją metody integrujące podejścia oparte na modelach i bez modeli w jednej architekturze \cite{Algo.I2A}. Model pozwala agentowi symulować wynik akcji podjętej w danym stanie. Główną zaletą jest to, że model pozwala agentowi planować, myśleć z wyprzedzeniem. Agent widzi co stanie się w przypadku różnych jego wyborów i może zdecydować o kolejnej akcji bez ryzyka negatywnych konsekwencji prób i błędów w rzeczywistym środowisku - w tym ryzyka podjęcia złej i nieodwracalnej decyzji. Nawet jeśli model musi zostać zsyntetyzowany na podstawie rzeczywistych doświadczeń z przeszłości, można to zrobić bardziej efektywnie ze względu na dane, aniżelu w przypadku metod bez modelowania. Co więcej, ten sam model może być wykorzystany przez agenta do wykonania innych zadań w tym samym środowisku \cite{Algo.I2A}, a planowanie niesie obietnicę zwiększenia wydajności tylko poprzez zwiększenie budżetu obliczeniowego na wyszukiwanie korzystnych akcji \cite{Algo.AlphaGoZero}.

Metody bez modelowania są bardziej popularne i zostały w większym stopniu przebadane i przetestowane niż metody oparte na modelach. Podczas gdy metody bez modelowania rezygnują z potencjalnego wzrostu wydajności, są one łatwiejsze do wdrożenia i dostrojenia. Głównym minusem uczenia ze wzmacnieniem opartego na modelowaniu jest fakt, że model środowiska zwykle nie jest dostępny dla agenta. Jeśli agent chce w tym przypadku użyć modelu, musi zamodelować swoje środowisko na podstawie własnych doświadczeń, co stwarza kilka wyzwań. Ponieważ model jest często tylko przybliżeniem rzeczywistego środowiska, jednym z nich jest obciążenie modelu, które można rozumieć jako niedokładne przewidywanie przyszłości. Może ono zostać wykorzystane przez agenta \cite{Algo.WorldModels} w wyniku czego agent działa dobrze w odniesieniu do modelu, ale zachowuje się nieoptymalnie w rzeczywistym środowisku. Inne wyzwania wynikają również z fundamentalnych wad przybliżenia modelu świata. Wydajność agentów wykorzystujących typowe metody planowania zwykle bardzo spada w obliczu pozornie niewielkich błędów modelu \cite{Study.PlanWithImperfectModel}. Błędy występujące podczas planowania kumulują się, powodując coraz bardziej niedokładne przewidywania, co w przypadku dalszego horyzontu planu może mieć katastrofalne skutki \cite{Study.CompoundingModelError}.

Istnieje wiele rzeczywistych problemów, które mogą skorzystać z zastosowania ogólnego systemu sztucznej inteligencji. Firma o nazwie DeepMind, bazująca na swoim doświadczeniu w tworzeniu mistrzowskiego algorytmu AlphaZero \cite{Algo.AlphaZero} dla gry Go, opublikowała AlphaFold \cite{Algo.AlphaFold}. System ten może przewidzieć strukturę białek na podstawie ich budowy chemicznej. Modele 3D białek generowanych przez AlphaFold są znacznie dokładniejsze niż modele wytworzone przez jakiekolwiek inne algorytmy. Synteza białek jest jednym z podstawowych wyzwań w biologii i ma ogromne znaczenie dla medycyny przyszłości.

Celem tej pracy jest zaproponowanie systemy głębokiego uczenia ze wzmocnieniem opartego na modelowaniu, który mógłby uczyć się rozwiązywać złożone zadania w skomplikowanych środowiskach z wielowymiarowymi obserwacjami. Rozwiązanie to ma bazować na najnowocześniejszych metodach uczenia modeli \cite{Algo.RecurrentEnvSim}\cite{Algo.JointFrameRewardPrediction}\cite{Algo.FastGenerativeModels} i opierać się na ostanich dokonaniach w dziedzinie uczenia ze wzmocnieniem opartego na modelowaniu \cite{Algo.SimPLe}\cite{Algo.VPN }\cite{Algo.WorldModels}\cite{Algo.PlaNet}. Końcowe rozwiązanie zostało nazwane ``Discrete PlaNet'' (DPN). Pomimo wielu trudności, osiągnęło ono poziom wydajności równy lub wyższy od silnych metod bazowych w warunkach małej ilości danych do jednego miliona interakcji z rzeczywistym środowiskiem.

Najtrudniejszą częścią tej pracy, niedocenianą początkowo przez autora, była synteza modelu. Obecne najnowocześniejsze metody uczenia modeli, choć prezentują obiecujące wyniki, nie zostały przetestowane pod kątem planowania z ich wykorzystaniem. Pod tym względem, praca ta wkroczyła na słabo przebadany teren i osiągnęła swój skromny sukces.

Przyszłe prace powinny skupić się na rozszerzeniu tej metody na inne wymagające zadania, takie jak: środowiska z rzadkimi nagrodami np. gra Freeway oraz złożone gry logiczne z ogromnymi przestrzeniami stanów np. Sokoban.
DPN, w przeciwieństwie do innej zaproponowanej metody o nazwie SimPLe \cite{Algo.SimPLe}, umożliwia zwiększenie wydajności poprzez zwiększenie budżetu obliczeniowego dla planowania. Tę hipotezę również należałoby poddać testom.
Ponadto wartościowym eksperymentem byłoby zbadanie w jakim stopniu model świata używany przez DPN do rozwiązywania jednego problemu można użyć do rozwiązania innych zadania w tym samym środowisku lub bardzo podobnych środowiskach.

\vspace{1cm}
\noindent
\textbf{Słowa kluczowe:} uczenie głębokie, uczenie ze wzmocnieniem, planowanie i uczenie, metody bazujące na symulacji i przeszukiwaniu, Arcade Learning Environment

\vspace{1cm}
\noindent
\textbf{Dziedziny nauki i techniki zgodne z wymogami OECD:} Nauki o komputerach i informatyka
